{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning CSV data...\n",
      "Number of classes: 21\n",
      "Label mapping: {0: 'acne', 1: 'acne-vulgaris', 2: 'actinic-keratosis', 3: 'basal-cell-carcinoma', 4: 'basal-cell-carcinoma-morpheiform', 5: 'dermatofibroma', 6: 'dermatomyositis', 7: 'dyshidrotic-eczema', 8: 'eczema', 9: 'epidermal-nevus', 10: 'folliculitis', 11: 'kaposi-sarcoma', 12: 'keloid', 13: 'malignant-melanoma', 14: 'melanoma', 15: 'mycosis-fungoides', 16: 'prurigo-nodularis', 17: 'pyogenic-granuloma', 18: 'seborrheic-keratosis', 19: 'squamous-cell-carcinoma', 20: 'superficial-spreading-melanoma-ssm'}\n",
      "Class distribution:\n",
      " label_numerical\n",
      "19    407\n",
      "3     328\n",
      "10    237\n",
      "1     234\n",
      "14    181\n",
      "8     143\n",
      "15    127\n",
      "0     127\n",
      "2     122\n",
      "16    119\n",
      "11    109\n",
      "12    109\n",
      "6     106\n",
      "20     83\n",
      "17     79\n",
      "13     78\n",
      "9      64\n",
      "7      58\n",
      "5      55\n",
      "18     47\n",
      "4      43\n",
      "Name: count, dtype: int64\n",
      "Loading and processing training images...\n",
      "Shape of X: (2856, 224, 224, 3)\n",
      "Shape of y: (2856,)\n",
      "Shape of fitzpatrick_data: (2856,)\n",
      "Setting up data augmentation...\n",
      "Class weights: {0: 1.0662932, 1: 0.58161443, 2: 1.1098154, 3: 0.41512176, 4: 3.1988795, 5: 2.4718614, 6: 1.2795519, 7: 2.3643892, 8: 0.9540518, 9: 2.1325865, 10: 0.5724311, 11: 1.2501369, 12: 1.2501369, 13: 1.7542243, 14: 0.7500821, 15: 1.0662932, 16: 1.1448622, 17: 1.7263794, 18: 2.8621554, 19: 0.33362547, 20: 1.6479076}\n",
      "Epoch 1/2\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 3s/step - accuracy: 0.0454 - loss: 4.1583 - val_accuracy: 0.1084 - val_loss: 3.1050\n",
      "Epoch 2/2\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 3s/step - accuracy: 0.0595 - loss: 3.9660 - val_accuracy: 0.0612 - val_loss: 3.1816\n",
      "Direct training successful, now trying with generator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Error during training: Graph execution error:\n",
      "\n",
      "Detected at node PyFunc defined at (most recent call last):\n",
      "<stack traces unavailable>\n",
      "TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float64, tf.float32), but the yielded element was (array([[[[0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         ...,\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ]],\n",
      "\n",
      "        [[0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         ...,\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ]],\n",
      "\n",
      "        [[0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         ...,\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.41234666, 0.29357198, 0.22850591],\n",
      "         [0.5839894 , 0.46141917, 0.39102218],\n",
      "         [0.653857  , 0.5244388 , 0.44987836],\n",
      "         ...,\n",
      "         [0.5016014 , 0.31740746, 0.2232898 ],\n",
      "         [0.50190556, 0.31761023, 0.22349258],\n",
      "         [0.5020438 , 0.31764707, 0.22361238]],\n",
      "\n",
      "        [[0.38159934, 0.27732018, 0.2140594 ],\n",
      "         [0.4799936 , 0.36979708, 0.3009131 ],\n",
      "         [0.49705333, 0.37765977, 0.30638885],\n",
      "         ...,\n",
      "         [0.4899591 , 0.30956694, 0.21556778],\n",
      "         [0.49016187, 0.30976972, 0.21566917],\n",
      "         [0.4904489 , 0.30997247, 0.21585482]],\n",
      "\n",
      "        [[0.5128784 , 0.41953948, 0.3578004 ],\n",
      "         [0.4684157 , 0.3674446 , 0.30127278],\n",
      "         [0.3676794 , 0.25795034, 0.19007425],\n",
      "         ...,\n",
      "         [0.48223576, 0.3018436 , 0.21164753],\n",
      "         [0.48233715, 0.301945  , 0.21174891],\n",
      "         [0.4825241 , 0.30213195, 0.21185029]]],\n",
      "\n",
      "\n",
      "       [[[0.40329194, 0.34054685, 0.28172332],\n",
      "         [0.40237215, 0.33962706, 0.28080353],\n",
      "         [0.40145233, 0.33870724, 0.2798837 ],\n",
      "         ...,\n",
      "         [0.31594402, 0.27962655, 0.24041086],\n",
      "         [0.3342473 , 0.29594555, 0.25398803],\n",
      "         [0.3356458 , 0.29954714, 0.24937129]],\n",
      "\n",
      "        [[0.40392157, 0.34117648, 0.28235295],\n",
      "         [0.40392157, 0.34117648, 0.28235295],\n",
      "         [0.40392157, 0.34117648, 0.28235295],\n",
      "         ...,\n",
      "         [0.32146284, 0.28422555, 0.24500987],\n",
      "         [0.3351671 , 0.29778516, 0.2530682 ],\n",
      "         [0.3338062 , 0.2967877 , 0.24753168]],\n",
      "\n",
      "        [[0.39777526, 0.33503017, 0.27620664],\n",
      "         [0.39961487, 0.33686978, 0.27804625],\n",
      "         [0.40145448, 0.33870938, 0.27988586],\n",
      "         ...,\n",
      "         [0.32698163, 0.2888246 , 0.24960889],\n",
      "         [0.3360869 , 0.29962477, 0.25214842],\n",
      "         [0.33196658, 0.2940283 , 0.24569207]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2061506 , 0.15704158, 0.10606118],\n",
      "         [0.22719222, 0.17137457, 0.11947847],\n",
      "         [0.24163462, 0.18200478, 0.1254902 ],\n",
      "         ...,\n",
      "         [0.09411765, 0.10980392, 0.10588235],\n",
      "         [0.09411765, 0.10980392, 0.10588235],\n",
      "         [0.09411765, 0.10980392, 0.10588235]],\n",
      "\n",
      "        [[0.21166942, 0.1607208 , 0.10974039],\n",
      "         [0.23087144, 0.17413397, 0.12131807],\n",
      "         [0.24439402, 0.18384439, 0.1254902 ],\n",
      "         ...,\n",
      "         [0.08775606, 0.10344233, 0.09952077],\n",
      "         [0.08959567, 0.10528194, 0.10136037],\n",
      "         [0.09143528, 0.10712155, 0.10319998]],\n",
      "\n",
      "        [[0.21718822, 0.1644    , 0.1134196 ],\n",
      "         [0.23455065, 0.17689338, 0.12315768],\n",
      "         [0.24715343, 0.18568398, 0.1254902 ],\n",
      "         ...,\n",
      "         [0.07706032, 0.09274659, 0.08882502],\n",
      "         [0.07981972, 0.095506  , 0.09158443],\n",
      "         [0.08257914, 0.09826541, 0.09434384]]],\n",
      "\n",
      "\n",
      "       [[[0.63162625, 0.52182233, 0.43554783],\n",
      "         [0.63930255, 0.52949864, 0.4432241 ],\n",
      "         [0.6450581 , 0.53909576, 0.4509004 ],\n",
      "         ...,\n",
      "         [0.55497295, 0.4862745 , 0.45490196],\n",
      "         [0.5541684 , 0.4862745 , 0.45490196],\n",
      "         [0.55336386, 0.4862745 , 0.45490196]],\n",
      "\n",
      "        [[0.63001716, 0.52021325, 0.4339387 ],\n",
      "         [0.63769346, 0.52788955, 0.441615  ],\n",
      "         [0.6442535 , 0.53668207, 0.4492913 ],\n",
      "         ...,\n",
      "         [0.5547476 , 0.49169374, 0.4603212 ],\n",
      "         [0.5555522 , 0.4941074 , 0.46273485],\n",
      "         [0.5563567 , 0.49652106, 0.4651485 ]],\n",
      "\n",
      "        [[0.628408  , 0.5186041 , 0.4323296 ],\n",
      "         [0.6360843 , 0.5262804 , 0.4400059 ],\n",
      "         [0.64344895, 0.5342684 , 0.44768217],\n",
      "         ...,\n",
      "         [0.56030875, 0.50320816, 0.47011265],\n",
      "         [0.56191784, 0.50562185, 0.47172177],\n",
      "         [0.563527  , 0.50803554, 0.47333086]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.6919217 , 0.6330982 , 0.5585884 ],\n",
      "         [0.69205844, 0.6332349 , 0.5587251 ],\n",
      "         [0.68952185, 0.6306983 , 0.5561885 ],\n",
      "         ...,\n",
      "         [0.60784316, 0.52156866, 0.4685901 ],\n",
      "         [0.60784316, 0.52156866, 0.47058824],\n",
      "         [0.60608655, 0.51981205, 0.46883163]],\n",
      "\n",
      "        [[0.6908332 , 0.6320097 , 0.5574999 ],\n",
      "         [0.6892241 , 0.6304006 , 0.5558908 ],\n",
      "         [0.6869824 , 0.62815887, 0.55364907],\n",
      "         ...,\n",
      "         [0.60784316, 0.52156866, 0.46778557],\n",
      "         [0.60784316, 0.52156866, 0.47058824],\n",
      "         [0.6068911 , 0.5206166 , 0.4696362 ]],\n",
      "\n",
      "        [[0.6831569 , 0.6243334 , 0.5498236 ],\n",
      "         [0.6815478 , 0.6227243 , 0.5482145 ],\n",
      "         [0.67993873, 0.6211152 , 0.5466054 ],\n",
      "         ...,\n",
      "         [0.60784316, 0.52156866, 0.466981  ],\n",
      "         [0.60784316, 0.52156866, 0.47058824],\n",
      "         [0.6076957 , 0.5214212 , 0.47044075]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.33242732, 0.2500744 , 0.17767413],\n",
      "         [0.3276624 , 0.24530946, 0.1660034 ],\n",
      "         [0.32241198, 0.24005905, 0.15770611],\n",
      "         ...,\n",
      "         [0.53333336, 0.4862745 , 0.38431373],\n",
      "         [0.53333336, 0.4862745 , 0.38431373],\n",
      "         [0.53333336, 0.4862745 , 0.38431373]],\n",
      "\n",
      "        [[0.33292204, 0.2505691 , 0.17915821],\n",
      "         [0.3286518 , 0.24629885, 0.16748749],\n",
      "         [0.3219173 , 0.23956434, 0.15721141],\n",
      "         ...,\n",
      "         [0.5430284 , 0.4959696 , 0.3940088 ],\n",
      "         [0.5455019 , 0.49844307, 0.3964823 ],\n",
      "         [0.5479754 , 0.5009166 , 0.39895576]],\n",
      "\n",
      "        [[0.3335835 , 0.25123057, 0.18055892],\n",
      "         [0.32952648, 0.24717353, 0.16897158],\n",
      "         [0.3218607 , 0.23950776, 0.15730086],\n",
      "         ...,\n",
      "         [0.56057197, 0.51351315, 0.4115523 ],\n",
      "         [0.5625507 , 0.5154919 , 0.4135311 ],\n",
      "         [0.56452954, 0.5174707 , 0.4155099 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.7193455 , 0.61346316, 0.5708817 ],\n",
      "         [0.7213243 , 0.615442  , 0.5723658 ],\n",
      "         [0.7254712 , 0.6200224 , 0.57558435],\n",
      "         ...,\n",
      "         [0.8034976 , 0.6616398 , 0.4939511 ],\n",
      "         [0.7739958 , 0.6266277 , 0.44272795],\n",
      "         [0.7428739 , 0.5960931 , 0.394981  ]],\n",
      "\n",
      "        [[0.7515788 , 0.6490309 , 0.5958903 ],\n",
      "         [0.75603104, 0.6539779 , 0.59935313],\n",
      "         [0.76329935, 0.6609363 , 0.60603434],\n",
      "         ...,\n",
      "         [0.8069605 , 0.6660921 , 0.4998875 ],\n",
      "         [0.7779534 , 0.6310799 , 0.4496537 ],\n",
      "         [0.7468315 , 0.59955597, 0.40042266]],\n",
      "\n",
      "        [[0.8097129 , 0.70444894, 0.649547  ],\n",
      "         [0.817628  , 0.7118694 , 0.65696746],\n",
      "         [0.8225755 , 0.71706414, 0.6617912 ],\n",
      "         ...,\n",
      "         [0.8104234 , 0.6705444 , 0.50582385],\n",
      "         [0.78191096, 0.6355322 , 0.45657948],\n",
      "         [0.75078905, 0.6030189 , 0.40586436]]],\n",
      "\n",
      "\n",
      "       [[[0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         ...,\n",
      "         [0.94590265, 0.610257  , 0.4989454 ],\n",
      "         [0.9497912 , 0.61960787, 0.51841867],\n",
      "         [0.94998723, 0.61813086, 0.5193532 ]],\n",
      "\n",
      "        [[0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         ...,\n",
      "         [0.94641083, 0.6117816 , 0.5019945 ],\n",
      "         [0.9502993 , 0.61960787, 0.5189268 ],\n",
      "         [0.9479545 , 0.61711454, 0.51782864]],\n",
      "\n",
      "        [[0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         ...,\n",
      "         [0.946919  , 0.61330616, 0.5050436 ],\n",
      "         [0.9508075 , 0.61960787, 0.519435  ],\n",
      "         [0.9459217 , 0.61609817, 0.5163041 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.02745098, 0.03022547, 0.04983331],\n",
      "         [0.02745098, 0.03137255, 0.05098039],\n",
      "         [0.03015934, 0.0340809 , 0.05368875],\n",
      "         ...,\n",
      "         [0.83502907, 0.5329358 , 0.4545044 ],\n",
      "         [0.8367968 , 0.53446037, 0.45602897],\n",
      "         [0.8388295 , 0.53598493, 0.45755354]],\n",
      "\n",
      "        [[0.02745098, 0.03073366, 0.0503415 ],\n",
      "         [0.02745098, 0.03137255, 0.05098039],\n",
      "         [0.03066752, 0.03458909, 0.05419694],\n",
      "         ...,\n",
      "         [0.8283461 , 0.52246374, 0.44393286],\n",
      "         [0.82826847, 0.52279484, 0.44436347],\n",
      "         [0.82928485, 0.5243194 , 0.445888  ]],\n",
      "\n",
      "        [[0.02745098, 0.03124184, 0.05084968],\n",
      "         [0.02745098, 0.03137255, 0.05098039],\n",
      "         [0.03117571, 0.03509728, 0.05470512],\n",
      "         ...,\n",
      "         [0.86307704, 0.5571947 , 0.47477534],\n",
      "         [0.8587689 , 0.55288655, 0.4709754 ],\n",
      "         [0.85419524, 0.5483129 , 0.46690992]]],\n",
      "\n",
      "\n",
      "       [[[0.69897634, 0.56172144, 0.48329002],\n",
      "         [0.7046688 , 0.5665113 , 0.49078786],\n",
      "         [0.7163303 , 0.5751538 , 0.5093554 ],\n",
      "         ...,\n",
      "         [0.64582545, 0.52033526, 0.43798226],\n",
      "         [0.64686215, 0.52137196, 0.43901896],\n",
      "         [0.64705884, 0.52156866, 0.4392157 ]],\n",
      "\n",
      "        [[0.69845796, 0.56120306, 0.48277166],\n",
      "         [0.70311373, 0.5654745 , 0.48819607],\n",
      "         [0.7147752 , 0.57359874, 0.507282  ],\n",
      "         ...,\n",
      "         [0.64705884, 0.52156866, 0.4392157 ],\n",
      "         [0.64705884, 0.52156866, 0.4392157 ],\n",
      "         [0.64667326, 0.5211831 , 0.4388301 ]],\n",
      "\n",
      "        [[0.6981389 , 0.56108326, 0.4822533 ],\n",
      "         [0.70182675, 0.56457186, 0.48614046],\n",
      "         [0.7132202 , 0.57221216, 0.5050401 ],\n",
      "         ...,\n",
      "         [0.6438228 , 0.5183326 , 0.43597966],\n",
      "         [0.64330447, 0.5178143 , 0.43546128],\n",
      "         [0.6427861 , 0.5172959 , 0.43494293]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.72428113, 0.58702624, 0.5085949 ],\n",
      "         [0.7247995 , 0.5875446 , 0.50911325],\n",
      "         [0.72531784, 0.58806294, 0.5096316 ],\n",
      "         ...,\n",
      "         [0.6826977 , 0.5454428 , 0.45916826],\n",
      "         [0.6868952 , 0.5496403 , 0.46336576],\n",
      "         [0.6943936 , 0.5571387 , 0.47086415]],\n",
      "\n",
      "        [[0.7281683 , 0.5909134 , 0.51248205],\n",
      "         [0.7286867 , 0.5914318 , 0.5130004 ],\n",
      "         [0.729205  , 0.5919501 , 0.51351875],\n",
      "         ...,\n",
      "         [0.68235296, 0.54509807, 0.45882353],\n",
      "         [0.6860665 , 0.5488116 , 0.46253708],\n",
      "         [0.6936328 , 0.5563779 , 0.47010335]],\n",
      "\n",
      "        [[0.7320555 , 0.5948006 , 0.5163692 ],\n",
      "         [0.7325738 , 0.5953189 , 0.51688755],\n",
      "         [0.7330922 , 0.5958373 , 0.5174059 ],\n",
      "         ...,\n",
      "         [0.68235296, 0.54509807, 0.45882353],\n",
      "         [0.6855481 , 0.54829323, 0.46201873],\n",
      "         [0.6925961 , 0.5553412 , 0.46906665]]]], dtype=float32), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.]])).\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 204, in generator_py_func\n",
      "    flattened_values = nest.flatten_up_to(output_types, values)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\", line 237, in flatten_up_to\n",
      "    return nest_util.flatten_up_to(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1541, in flatten_up_to\n",
      "    return _tf_data_flatten_up_to(shallow_tree, input_tree)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1570, in _tf_data_flatten_up_to\n",
      "    _tf_data_assert_shallow_structure(shallow_tree, input_tree)\n",
      "\n",
      "  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1427, in _tf_data_assert_shallow_structure\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: The two structures don't have the same sequence length. Input structure has length 2, while shallow structure has length 3.\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "          ^^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 206, in generator_py_func\n",
      "    raise TypeError(\n",
      "\n",
      "TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float64, tf.float32), but the yielded element was (array([[[[0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         ...,\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ]],\n",
      "\n",
      "        [[0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         ...,\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ]],\n",
      "\n",
      "        [[0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         ...,\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ],\n",
      "         [0.7254902 , 0.7411765 , 0.7764706 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.41234666, 0.29357198, 0.22850591],\n",
      "         [0.5839894 , 0.46141917, 0.39102218],\n",
      "         [0.653857  , 0.5244388 , 0.44987836],\n",
      "         ...,\n",
      "         [0.5016014 , 0.31740746, 0.2232898 ],\n",
      "         [0.50190556, 0.31761023, 0.22349258],\n",
      "         [0.5020438 , 0.31764707, 0.22361238]],\n",
      "\n",
      "        [[0.38159934, 0.27732018, 0.2140594 ],\n",
      "         [0.4799936 , 0.36979708, 0.3009131 ],\n",
      "         [0.49705333, 0.37765977, 0.30638885],\n",
      "         ...,\n",
      "         [0.4899591 , 0.30956694, 0.21556778],\n",
      "         [0.49016187, 0.30976972, 0.21566917],\n",
      "         [0.4904489 , 0.30997247, 0.21585482]],\n",
      "\n",
      "        [[0.5128784 , 0.41953948, 0.3578004 ],\n",
      "         [0.4684157 , 0.3674446 , 0.30127278],\n",
      "         [0.3676794 , 0.25795034, 0.19007425],\n",
      "         ...,\n",
      "         [0.48223576, 0.3018436 , 0.21164753],\n",
      "         [0.48233715, 0.301945  , 0.21174891],\n",
      "         [0.4825241 , 0.30213195, 0.21185029]]],\n",
      "\n",
      "\n",
      "       [[[0.40329194, 0.34054685, 0.28172332],\n",
      "         [0.40237215, 0.33962706, 0.28080353],\n",
      "         [0.40145233, 0.33870724, 0.2798837 ],\n",
      "         ...,\n",
      "         [0.31594402, 0.27962655, 0.24041086],\n",
      "         [0.3342473 , 0.29594555, 0.25398803],\n",
      "         [0.3356458 , 0.29954714, 0.24937129]],\n",
      "\n",
      "        [[0.40392157, 0.34117648, 0.28235295],\n",
      "         [0.40392157, 0.34117648, 0.28235295],\n",
      "         [0.40392157, 0.34117648, 0.28235295],\n",
      "         ...,\n",
      "         [0.32146284, 0.28422555, 0.24500987],\n",
      "         [0.3351671 , 0.29778516, 0.2530682 ],\n",
      "         [0.3338062 , 0.2967877 , 0.24753168]],\n",
      "\n",
      "        [[0.39777526, 0.33503017, 0.27620664],\n",
      "         [0.39961487, 0.33686978, 0.27804625],\n",
      "         [0.40145448, 0.33870938, 0.27988586],\n",
      "         ...,\n",
      "         [0.32698163, 0.2888246 , 0.24960889],\n",
      "         [0.3360869 , 0.29962477, 0.25214842],\n",
      "         [0.33196658, 0.2940283 , 0.24569207]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2061506 , 0.15704158, 0.10606118],\n",
      "         [0.22719222, 0.17137457, 0.11947847],\n",
      "         [0.24163462, 0.18200478, 0.1254902 ],\n",
      "         ...,\n",
      "         [0.09411765, 0.10980392, 0.10588235],\n",
      "         [0.09411765, 0.10980392, 0.10588235],\n",
      "         [0.09411765, 0.10980392, 0.10588235]],\n",
      "\n",
      "        [[0.21166942, 0.1607208 , 0.10974039],\n",
      "         [0.23087144, 0.17413397, 0.12131807],\n",
      "         [0.24439402, 0.18384439, 0.1254902 ],\n",
      "         ...,\n",
      "         [0.08775606, 0.10344233, 0.09952077],\n",
      "         [0.08959567, 0.10528194, 0.10136037],\n",
      "         [0.09143528, 0.10712155, 0.10319998]],\n",
      "\n",
      "        [[0.21718822, 0.1644    , 0.1134196 ],\n",
      "         [0.23455065, 0.17689338, 0.12315768],\n",
      "         [0.24715343, 0.18568398, 0.1254902 ],\n",
      "         ...,\n",
      "         [0.07706032, 0.09274659, 0.08882502],\n",
      "         [0.07981972, 0.095506  , 0.09158443],\n",
      "         [0.08257914, 0.09826541, 0.09434384]]],\n",
      "\n",
      "\n",
      "       [[[0.63162625, 0.52182233, 0.43554783],\n",
      "         [0.63930255, 0.52949864, 0.4432241 ],\n",
      "         [0.6450581 , 0.53909576, 0.4509004 ],\n",
      "         ...,\n",
      "         [0.55497295, 0.4862745 , 0.45490196],\n",
      "         [0.5541684 , 0.4862745 , 0.45490196],\n",
      "         [0.55336386, 0.4862745 , 0.45490196]],\n",
      "\n",
      "        [[0.63001716, 0.52021325, 0.4339387 ],\n",
      "         [0.63769346, 0.52788955, 0.441615  ],\n",
      "         [0.6442535 , 0.53668207, 0.4492913 ],\n",
      "         ...,\n",
      "         [0.5547476 , 0.49169374, 0.4603212 ],\n",
      "         [0.5555522 , 0.4941074 , 0.46273485],\n",
      "         [0.5563567 , 0.49652106, 0.4651485 ]],\n",
      "\n",
      "        [[0.628408  , 0.5186041 , 0.4323296 ],\n",
      "         [0.6360843 , 0.5262804 , 0.4400059 ],\n",
      "         [0.64344895, 0.5342684 , 0.44768217],\n",
      "         ...,\n",
      "         [0.56030875, 0.50320816, 0.47011265],\n",
      "         [0.56191784, 0.50562185, 0.47172177],\n",
      "         [0.563527  , 0.50803554, 0.47333086]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.6919217 , 0.6330982 , 0.5585884 ],\n",
      "         [0.69205844, 0.6332349 , 0.5587251 ],\n",
      "         [0.68952185, 0.6306983 , 0.5561885 ],\n",
      "         ...,\n",
      "         [0.60784316, 0.52156866, 0.4685901 ],\n",
      "         [0.60784316, 0.52156866, 0.47058824],\n",
      "         [0.60608655, 0.51981205, 0.46883163]],\n",
      "\n",
      "        [[0.6908332 , 0.6320097 , 0.5574999 ],\n",
      "         [0.6892241 , 0.6304006 , 0.5558908 ],\n",
      "         [0.6869824 , 0.62815887, 0.55364907],\n",
      "         ...,\n",
      "         [0.60784316, 0.52156866, 0.46778557],\n",
      "         [0.60784316, 0.52156866, 0.47058824],\n",
      "         [0.6068911 , 0.5206166 , 0.4696362 ]],\n",
      "\n",
      "        [[0.6831569 , 0.6243334 , 0.5498236 ],\n",
      "         [0.6815478 , 0.6227243 , 0.5482145 ],\n",
      "         [0.67993873, 0.6211152 , 0.5466054 ],\n",
      "         ...,\n",
      "         [0.60784316, 0.52156866, 0.466981  ],\n",
      "         [0.60784316, 0.52156866, 0.47058824],\n",
      "         [0.6076957 , 0.5214212 , 0.47044075]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.33242732, 0.2500744 , 0.17767413],\n",
      "         [0.3276624 , 0.24530946, 0.1660034 ],\n",
      "         [0.32241198, 0.24005905, 0.15770611],\n",
      "         ...,\n",
      "         [0.53333336, 0.4862745 , 0.38431373],\n",
      "         [0.53333336, 0.4862745 , 0.38431373],\n",
      "         [0.53333336, 0.4862745 , 0.38431373]],\n",
      "\n",
      "        [[0.33292204, 0.2505691 , 0.17915821],\n",
      "         [0.3286518 , 0.24629885, 0.16748749],\n",
      "         [0.3219173 , 0.23956434, 0.15721141],\n",
      "         ...,\n",
      "         [0.5430284 , 0.4959696 , 0.3940088 ],\n",
      "         [0.5455019 , 0.49844307, 0.3964823 ],\n",
      "         [0.5479754 , 0.5009166 , 0.39895576]],\n",
      "\n",
      "        [[0.3335835 , 0.25123057, 0.18055892],\n",
      "         [0.32952648, 0.24717353, 0.16897158],\n",
      "         [0.3218607 , 0.23950776, 0.15730086],\n",
      "         ...,\n",
      "         [0.56057197, 0.51351315, 0.4115523 ],\n",
      "         [0.5625507 , 0.5154919 , 0.4135311 ],\n",
      "         [0.56452954, 0.5174707 , 0.4155099 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.7193455 , 0.61346316, 0.5708817 ],\n",
      "         [0.7213243 , 0.615442  , 0.5723658 ],\n",
      "         [0.7254712 , 0.6200224 , 0.57558435],\n",
      "         ...,\n",
      "         [0.8034976 , 0.6616398 , 0.4939511 ],\n",
      "         [0.7739958 , 0.6266277 , 0.44272795],\n",
      "         [0.7428739 , 0.5960931 , 0.394981  ]],\n",
      "\n",
      "        [[0.7515788 , 0.6490309 , 0.5958903 ],\n",
      "         [0.75603104, 0.6539779 , 0.59935313],\n",
      "         [0.76329935, 0.6609363 , 0.60603434],\n",
      "         ...,\n",
      "         [0.8069605 , 0.6660921 , 0.4998875 ],\n",
      "         [0.7779534 , 0.6310799 , 0.4496537 ],\n",
      "         [0.7468315 , 0.59955597, 0.40042266]],\n",
      "\n",
      "        [[0.8097129 , 0.70444894, 0.649547  ],\n",
      "         [0.817628  , 0.7118694 , 0.65696746],\n",
      "         [0.8225755 , 0.71706414, 0.6617912 ],\n",
      "         ...,\n",
      "         [0.8104234 , 0.6705444 , 0.50582385],\n",
      "         [0.78191096, 0.6355322 , 0.45657948],\n",
      "         [0.75078905, 0.6030189 , 0.40586436]]],\n",
      "\n",
      "\n",
      "       [[[0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         ...,\n",
      "         [0.94590265, 0.610257  , 0.4989454 ],\n",
      "         [0.9497912 , 0.61960787, 0.51841867],\n",
      "         [0.94998723, 0.61813086, 0.5193532 ]],\n",
      "\n",
      "        [[0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         ...,\n",
      "         [0.94641083, 0.6117816 , 0.5019945 ],\n",
      "         [0.9502993 , 0.61960787, 0.5189268 ],\n",
      "         [0.9479545 , 0.61711454, 0.51782864]],\n",
      "\n",
      "        [[0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         [0.04705882, 0.04313726, 0.0627451 ],\n",
      "         ...,\n",
      "         [0.946919  , 0.61330616, 0.5050436 ],\n",
      "         [0.9508075 , 0.61960787, 0.519435  ],\n",
      "         [0.9459217 , 0.61609817, 0.5163041 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.02745098, 0.03022547, 0.04983331],\n",
      "         [0.02745098, 0.03137255, 0.05098039],\n",
      "         [0.03015934, 0.0340809 , 0.05368875],\n",
      "         ...,\n",
      "         [0.83502907, 0.5329358 , 0.4545044 ],\n",
      "         [0.8367968 , 0.53446037, 0.45602897],\n",
      "         [0.8388295 , 0.53598493, 0.45755354]],\n",
      "\n",
      "        [[0.02745098, 0.03073366, 0.0503415 ],\n",
      "         [0.02745098, 0.03137255, 0.05098039],\n",
      "         [0.03066752, 0.03458909, 0.05419694],\n",
      "         ...,\n",
      "         [0.8283461 , 0.52246374, 0.44393286],\n",
      "         [0.82826847, 0.52279484, 0.44436347],\n",
      "         [0.82928485, 0.5243194 , 0.445888  ]],\n",
      "\n",
      "        [[0.02745098, 0.03124184, 0.05084968],\n",
      "         [0.02745098, 0.03137255, 0.05098039],\n",
      "         [0.03117571, 0.03509728, 0.05470512],\n",
      "         ...,\n",
      "         [0.86307704, 0.5571947 , 0.47477534],\n",
      "         [0.8587689 , 0.55288655, 0.4709754 ],\n",
      "         [0.85419524, 0.5483129 , 0.46690992]]],\n",
      "\n",
      "\n",
      "       [[[0.69897634, 0.56172144, 0.48329002],\n",
      "         [0.7046688 , 0.5665113 , 0.49078786],\n",
      "         [0.7163303 , 0.5751538 , 0.5093554 ],\n",
      "         ...,\n",
      "         [0.64582545, 0.52033526, 0.43798226],\n",
      "         [0.64686215, 0.52137196, 0.43901896],\n",
      "         [0.64705884, 0.52156866, 0.4392157 ]],\n",
      "\n",
      "        [[0.69845796, 0.56120306, 0.48277166],\n",
      "         [0.70311373, 0.5654745 , 0.48819607],\n",
      "         [0.7147752 , 0.57359874, 0.507282  ],\n",
      "         ...,\n",
      "         [0.64705884, 0.52156866, 0.4392157 ],\n",
      "         [0.64705884, 0.52156866, 0.4392157 ],\n",
      "         [0.64667326, 0.5211831 , 0.4388301 ]],\n",
      "\n",
      "        [[0.6981389 , 0.56108326, 0.4822533 ],\n",
      "         [0.70182675, 0.56457186, 0.48614046],\n",
      "         [0.7132202 , 0.57221216, 0.5050401 ],\n",
      "         ...,\n",
      "         [0.6438228 , 0.5183326 , 0.43597966],\n",
      "         [0.64330447, 0.5178143 , 0.43546128],\n",
      "         [0.6427861 , 0.5172959 , 0.43494293]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.72428113, 0.58702624, 0.5085949 ],\n",
      "         [0.7247995 , 0.5875446 , 0.50911325],\n",
      "         [0.72531784, 0.58806294, 0.5096316 ],\n",
      "         ...,\n",
      "         [0.6826977 , 0.5454428 , 0.45916826],\n",
      "         [0.6868952 , 0.5496403 , 0.46336576],\n",
      "         [0.6943936 , 0.5571387 , 0.47086415]],\n",
      "\n",
      "        [[0.7281683 , 0.5909134 , 0.51248205],\n",
      "         [0.7286867 , 0.5914318 , 0.5130004 ],\n",
      "         [0.729205  , 0.5919501 , 0.51351875],\n",
      "         ...,\n",
      "         [0.68235296, 0.54509807, 0.45882353],\n",
      "         [0.6860665 , 0.5488116 , 0.46253708],\n",
      "         [0.6936328 , 0.5563779 , 0.47010335]],\n",
      "\n",
      "        [[0.7320555 , 0.5948006 , 0.5163692 ],\n",
      "         [0.7325738 , 0.5953189 , 0.51688755],\n",
      "         [0.7330922 , 0.5958373 , 0.5174059 ],\n",
      "         ...,\n",
      "         [0.68235296, 0.54509807, 0.45882353],\n",
      "         [0.6855481 , 0.54829323, 0.46201873],\n",
      "         [0.6925961 , 0.5553412 , 0.46906665]]]], dtype=float32), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.]])).\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_576086]\n",
      "Trying alternative approach...\n",
      "Epoch 1/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0657 - loss: 3.8137\n",
      "Epoch 1: val_accuracy improved from -inf to 0.13811, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 3s/step - accuracy: 0.0658 - loss: 3.8133 - val_accuracy: 0.1381 - val_loss: 2.9744 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0971 - loss: 3.5976\n",
      "Epoch 2: val_accuracy improved from 0.13811 to 0.13986, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 3s/step - accuracy: 0.0971 - loss: 3.5977 - val_accuracy: 0.1399 - val_loss: 2.9314 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0999 - loss: 3.6169\n",
      "Epoch 3: val_accuracy improved from 0.13986 to 0.16434, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 3s/step - accuracy: 0.0999 - loss: 3.6161 - val_accuracy: 0.1643 - val_loss: 2.8139 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1156 - loss: 3.4684\n",
      "Epoch 4: val_accuracy improved from 0.16434 to 0.18881, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 3s/step - accuracy: 0.1157 - loss: 3.4679 - val_accuracy: 0.1888 - val_loss: 2.7916 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1314 - loss: 3.3459\n",
      "Epoch 5: val_accuracy improved from 0.18881 to 0.20280, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 3s/step - accuracy: 0.1315 - loss: 3.3458 - val_accuracy: 0.2028 - val_loss: 2.7065 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1350 - loss: 3.2624\n",
      "Epoch 6: val_accuracy did not improve from 0.20280\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 3s/step - accuracy: 0.1350 - loss: 3.2625 - val_accuracy: 0.1818 - val_loss: 2.7217 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1494 - loss: 3.1938\n",
      "Epoch 7: val_accuracy did not improve from 0.20280\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 3s/step - accuracy: 0.1494 - loss: 3.1935 - val_accuracy: 0.2028 - val_loss: 2.6957 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1444 - loss: 3.1675\n",
      "Epoch 8: val_accuracy improved from 0.20280 to 0.21853, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 3s/step - accuracy: 0.1445 - loss: 3.1673 - val_accuracy: 0.2185 - val_loss: 2.6097 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1722 - loss: 3.0313\n",
      "Epoch 9: val_accuracy improved from 0.21853 to 0.22028, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 3s/step - accuracy: 0.1722 - loss: 3.0315 - val_accuracy: 0.2203 - val_loss: 2.5659 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.1857 - loss: 3.0170\n",
      "Epoch 10: val_accuracy improved from 0.22028 to 0.24126, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 3s/step - accuracy: 0.1858 - loss: 3.0167 - val_accuracy: 0.2413 - val_loss: 2.5440 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.1855 - loss: 2.9770\n",
      "Epoch 11: val_accuracy did not improve from 0.24126\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 3s/step - accuracy: 0.1856 - loss: 2.9768 - val_accuracy: 0.2115 - val_loss: 2.5664 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.1871 - loss: 2.9659\n",
      "Epoch 12: val_accuracy improved from 0.24126 to 0.24301, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 3s/step - accuracy: 0.1871 - loss: 2.9657 - val_accuracy: 0.2430 - val_loss: 2.4901 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2230 - loss: 2.8300\n",
      "Epoch 13: val_accuracy did not improve from 0.24301\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 3s/step - accuracy: 0.2231 - loss: 2.8298 - val_accuracy: 0.2203 - val_loss: 2.5192 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2069 - loss: 2.8054\n",
      "Epoch 14: val_accuracy improved from 0.24301 to 0.25175, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 3s/step - accuracy: 0.2070 - loss: 2.8049 - val_accuracy: 0.2517 - val_loss: 2.5004 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2423 - loss: 2.7240\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.25175\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 3s/step - accuracy: 0.2423 - loss: 2.7235 - val_accuracy: 0.2483 - val_loss: 2.5157 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2546 - loss: 2.6313\n",
      "Epoch 16: val_accuracy improved from 0.25175 to 0.26573, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 3s/step - accuracy: 0.2546 - loss: 2.6310 - val_accuracy: 0.2657 - val_loss: 2.4345 - learning_rate: 5.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2639 - loss: 2.5900\n",
      "Epoch 17: val_accuracy improved from 0.26573 to 0.26748, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 3s/step - accuracy: 0.2640 - loss: 2.5897 - val_accuracy: 0.2675 - val_loss: 2.4585 - learning_rate: 5.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2840 - loss: 2.5210\n",
      "Epoch 18: val_accuracy improved from 0.26748 to 0.28147, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 3s/step - accuracy: 0.2841 - loss: 2.5204 - val_accuracy: 0.2815 - val_loss: 2.4310 - learning_rate: 5.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3026 - loss: 2.4193\n",
      "Epoch 19: val_accuracy did not improve from 0.28147\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 3s/step - accuracy: 0.3026 - loss: 2.4192 - val_accuracy: 0.2797 - val_loss: 2.4064 - learning_rate: 5.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3058 - loss: 2.3281\n",
      "Epoch 20: val_accuracy did not improve from 0.28147\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 3s/step - accuracy: 0.3059 - loss: 2.3280 - val_accuracy: 0.2762 - val_loss: 2.3878 - learning_rate: 5.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3390 - loss: 2.2856\n",
      "Epoch 21: val_accuracy improved from 0.28147 to 0.29021, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 3s/step - accuracy: 0.3391 - loss: 2.2854 - val_accuracy: 0.2902 - val_loss: 2.3944 - learning_rate: 5.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3343 - loss: 2.2404\n",
      "Epoch 22: val_accuracy did not improve from 0.29021\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 3s/step - accuracy: 0.3344 - loss: 2.2403 - val_accuracy: 0.2780 - val_loss: 2.3828 - learning_rate: 5.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3442 - loss: 2.2578\n",
      "Epoch 23: val_accuracy improved from 0.29021 to 0.29545, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 3s/step - accuracy: 0.3443 - loss: 2.2572 - val_accuracy: 0.2955 - val_loss: 2.3534 - learning_rate: 5.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3808 - loss: 2.1876\n",
      "Epoch 24: val_accuracy improved from 0.29545 to 0.30769, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 3s/step - accuracy: 0.3809 - loss: 2.1872 - val_accuracy: 0.3077 - val_loss: 2.3692 - learning_rate: 5.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3983 - loss: 2.0721\n",
      "Epoch 25: val_accuracy improved from 0.30769 to 0.31119, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 3s/step - accuracy: 0.3982 - loss: 2.0720 - val_accuracy: 0.3112 - val_loss: 2.3852 - learning_rate: 5.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4131 - loss: 2.0344\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.31119\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 3s/step - accuracy: 0.4130 - loss: 2.0343 - val_accuracy: 0.2902 - val_loss: 2.4545 - learning_rate: 5.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4015 - loss: 2.0484\n",
      "Epoch 27: val_accuracy did not improve from 0.31119\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 3s/step - accuracy: 0.4016 - loss: 2.0479 - val_accuracy: 0.2727 - val_loss: 2.4035 - learning_rate: 2.5000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4170 - loss: 1.9799\n",
      "Epoch 28: val_accuracy improved from 0.31119 to 0.31294, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 3s/step - accuracy: 0.4171 - loss: 1.9797 - val_accuracy: 0.3129 - val_loss: 2.3724 - learning_rate: 2.5000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4334 - loss: 1.9262\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.31294 to 0.31469, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 3s/step - accuracy: 0.4335 - loss: 1.9261 - val_accuracy: 0.3147 - val_loss: 2.3627 - learning_rate: 2.5000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4250 - loss: 1.9043\n",
      "Epoch 30: val_accuracy improved from 0.31469 to 0.31643, saving model to skin_condition_model_improved.keras\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 3s/step - accuracy: 0.4251 - loss: 1.9041 - val_accuracy: 0.3164 - val_loss: 2.3662 - learning_rate: 1.2500e-05\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Building simplified model with transfer learning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,783,535</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_12     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_28          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">786,944</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_29          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_30          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,397</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb3 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1536\u001b[0m)     │    \u001b[38;5;34m10,783,535\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_12     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_28          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │         \u001b[38;5;34m6,144\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m786,944\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_29          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_30          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_35 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)             │         \u001b[38;5;34m5,397\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,716,420</span> (44.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,716,420\u001b[0m (44.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,504,219</span> (43.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,504,219\u001b[0m (43.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">212,201</span> (828.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m212,201\u001b[0m (828.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nTypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float64, tf.float32), but the yielded element was (array([[[[0.08128925, 0.06952454, 0.11266179],\n         [0.08197479, 0.07021008, 0.11334734],\n         [0.08235294, 0.07058824, 0.11372549],\n         ...,\n         [0.02068717, 0.01676561, 0.04529592],\n         [0.0255673 , 0.02062679, 0.05607511],\n         [0.03233111, 0.02448797, 0.07346391]],\n\n        [[0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         [0.07879915, 0.06703445, 0.1101717 ],\n         ...,\n         [0.02137272, 0.01745115, 0.04666701],\n         [0.02693839, 0.02131233, 0.05950284],\n         [0.03301665, 0.02517352, 0.07552055]],\n\n        [[0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         ...,\n         [0.02205826, 0.0181367 , 0.0480381 ],\n         [0.02830948, 0.02199788, 0.06293057],\n         [0.0337022 , 0.02585906, 0.07757718]],\n\n        ...,\n\n        [[0.08652318, 0.04068866, 0.09828788],\n         [0.09007052, 0.04576935, 0.10183523],\n         [0.12978587, 0.08162352, 0.13934353],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]],\n\n        [[0.08446655, 0.03931756, 0.09623125],\n         [0.09692597, 0.05193925, 0.10869068],\n         [0.13732687, 0.08847897, 0.14551343],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]],\n\n        [[0.08240991, 0.03794647, 0.09417462],\n         [0.10378142, 0.05810916, 0.11554613],\n         [0.14486787, 0.09533442, 0.15168333],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]]],\n\n\n       [[[0.4249347 , 0.34504914, 0.3634233 ],\n         [0.42071962, 0.3416298 , 0.35952672],\n         [0.42186275, 0.34188476, 0.35930443],\n         ...,\n         [0.6831743 , 0.54199785, 0.58121353],\n         [0.68365157, 0.5424751 , 0.5816908 ],\n         [0.6841288 , 0.54295236, 0.58216804]],\n\n        [[0.41059265, 0.3360245 , 0.35173663],\n         [0.4100829 , 0.3355731 , 0.35289258],\n         [0.42132497, 0.34511667, 0.36296198],\n         ...,\n         [0.6838979 , 0.54113704, 0.5811449 ],\n         [0.6824662 , 0.5387508 , 0.579236  ],\n         [0.68103445, 0.5363646 , 0.577327  ]],\n\n        [[0.40744996, 0.33294016, 0.35137042],\n         [0.41065064, 0.33614084, 0.35705963],\n         [0.4271035 , 0.3525937 , 0.3743314 ],\n         ...,\n         [0.6905341 , 0.5415145 , 0.58465177],\n         [0.70055616, 0.55153656, 0.5946738 ],\n         [0.71057814, 0.56155854, 0.6046958 ]],\n\n        ...,\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.716917  , 0.65196496, 0.7247601 ],\n         [0.7167791 , 0.6479347 , 0.7224445 ],\n         [0.686169  , 0.61987805, 0.69438785]],\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.71453077, 0.650056  , 0.7223739 ],\n         [0.7186881 , 0.6503209 , 0.7248307 ],\n         [0.69141865, 0.6241732 , 0.698683  ]],\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.7121446 , 0.64814705, 0.71998775],\n         [0.720597  , 0.65270704, 0.72721684],\n         [0.6966683 , 0.6284684 , 0.7029782 ]]],\n\n\n       [[[0.5667194 , 0.39607844, 0.32347673],\n         [0.5663973 , 0.39607844, 0.3237988 ],\n         [0.56607527, 0.39607844, 0.32412085],\n         ...,\n         [0.9620221 , 0.8101107 , 0.76305187],\n         [0.96593046, 0.8296523 , 0.7825935 ],\n         [0.9710501 , 0.8491939 , 0.80092376]],\n\n        [[0.57262796, 0.39607844, 0.32156864],\n         [0.5719838 , 0.39607844, 0.32156864],\n         [0.57133967, 0.39607844, 0.32156864],\n         ...,\n         [0.9623442 , 0.811721  , 0.7646622 ],\n         [0.9662525 , 0.83126265, 0.7842038 ],\n         [0.97169423, 0.85080427, 0.802212  ]],\n\n        [[0.5804446 , 0.39806542, 0.32156864],\n         [0.5798004 , 0.39774334, 0.32156864],\n         [0.5791563 , 0.39742127, 0.32156864],\n         ...,\n         [0.9626663 , 0.8133314 , 0.7662726 ],\n         [0.9665746 , 0.832873  , 0.78581417],\n         [0.9723384 , 0.8524146 , 0.8035003 ]],\n\n        ...,\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.27477637, 0.23556069, 0.38065872],\n         [0.268774  , 0.22955829, 0.37465635],\n         [0.26666668, 0.22745098, 0.37254903]],\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.27413222, 0.23491654, 0.38001457],\n         [0.26845193, 0.22923622, 0.37433428],\n         [0.26666668, 0.22745098, 0.37254903]],\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.2734881 , 0.23427239, 0.37937045],\n         [0.26812986, 0.22891416, 0.3740122 ],\n         [0.26666668, 0.22745098, 0.37254903]]],\n\n\n       ...,\n\n\n       [[[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.76699823, 0.61128163, 0.47010514],\n         [0.7591694 , 0.60230666, 0.46113017],\n         [0.75134057, 0.5917167 , 0.45054024]],\n\n        [[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.7665246 , 0.61057127, 0.46939477],\n         [0.75869584, 0.6018331 , 0.46065658],\n         [0.750867  , 0.59100634, 0.44982988]],\n\n        [[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.76605105, 0.6098609 , 0.46868443],\n         [0.7582222 , 0.6013595 , 0.46018302],\n         [0.7503934 , 0.59029603, 0.4491195 ]],\n\n        ...,\n\n        [[0.5671408 , 0.6159844 , 0.6370248 ],\n         [0.558448  , 0.6129835 , 0.6404345 ],\n         [0.55991787, 0.6152005 , 0.6413666 ],\n         ...,\n         [0.60390294, 0.6509804 , 0.660775  ],\n         [0.60437655, 0.6509804 , 0.6610118 ],\n         [0.6048501 , 0.6509804 , 0.66124856]],\n\n        [[0.56798685, 0.6187748 , 0.640423  ],\n         [0.55052006, 0.607025  , 0.63298017],\n         [0.550446  , 0.60670817, 0.6321519 ],\n         ...,\n         [0.6       , 0.64901745, 0.6568606 ],\n         [0.6       , 0.64925426, 0.6570974 ],\n         [0.6       , 0.6494911 , 0.6573342 ]],\n\n        [[0.574855  , 0.624527  , 0.6463931 ],\n         [0.5522338 , 0.60807985, 0.6338604 ],\n         [0.5523607 , 0.60731983, 0.6325268 ],\n         ...,\n         [0.5980442 , 0.64510304, 0.6529462 ],\n         [0.598281  , 0.64533985, 0.653183  ],\n         [0.59851784, 0.64557666, 0.6534198 ]]],\n\n\n       [[[0.53646374, 0.3443069 , 0.21881668],\n         [0.5372549 , 0.34509805, 0.21960784],\n         [0.5372549 , 0.34509805, 0.21960784],\n         ...,\n         [0.64803743, 0.40735027, 0.23627274],\n         [0.6403957 , 0.39725846, 0.23137255],\n         [0.63529414, 0.39088678, 0.23264262]],\n\n        [[0.5340238 , 0.34186694, 0.21637674],\n         [0.5335259 , 0.341369  , 0.21587881],\n         [0.53440887, 0.34225202, 0.2167618 ],\n         ...,\n         [0.64627147, 0.4047013 , 0.23450674],\n         [0.63862973, 0.39549246, 0.23137255],\n         [0.63529414, 0.3900038 , 0.23352562]],\n\n        [[0.53843445, 0.34627756, 0.22078736],\n         [0.5369617 , 0.34480482, 0.2193146 ],\n         [0.5360787 , 0.3439218 , 0.21843162],\n         ...,\n         [0.6445055 , 0.4020523 , 0.23274076],\n         [0.63686377, 0.3937265 , 0.23137255],\n         [0.63529414, 0.38912082, 0.2344086 ]],\n\n        ...,\n\n        [[0.5497593 , 0.36788774, 0.26592696],\n         [0.56134427, 0.3809521 , 0.2789913 ],\n         [0.558745  , 0.3813333 , 0.27639204],\n         ...,\n         [0.4392157 , 0.26783058, 0.03946038],\n         [0.4392157 , 0.2651816 , 0.03592841],\n         [0.4392157 , 0.26253262, 0.03239643]],\n\n        [[0.5495928 , 0.3692006 , 0.2672398 ],\n         [0.56462073, 0.38427114, 0.28226778],\n         [0.556979  , 0.38045028, 0.27462605],\n         ...,\n         [0.45662558, 0.29089978, 0.07505538],\n         [0.45132762, 0.2847188 , 0.06534245],\n         [0.44602966, 0.27853787, 0.05562952]],\n\n        [[0.5531247 , 0.37273258, 0.2707718 ],\n         [0.56285477, 0.38338816, 0.2805018 ],\n         [0.5568628 , 0.3795673 , 0.27615955],\n         ...,\n         [0.48235172, 0.32324773, 0.11708491],\n         [0.47617078, 0.3153008 , 0.10737199],\n         [0.4699898 , 0.30735385, 0.09765906]]],\n\n\n       [[[0.00295865, 0.00295865, 0.00295865],\n         [0.003148  , 0.003148  , 0.003148  ],\n         [0.00333735, 0.00333735, 0.00333735],\n         ...,\n         [0.10675435, 0.04868467, 0.04718081],\n         [0.10609207, 0.04933306, 0.04988058],\n         [0.0921601 , 0.04314797, 0.04122626]],\n\n        [[0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         ...,\n         [0.1564236 , 0.0999303 , 0.09648446],\n         [0.14805317, 0.09609426, 0.09350255],\n         [0.12336127, 0.07512596, 0.07179263]],\n\n        [[0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         ...,\n         [0.2070707 , 0.15295699, 0.14903542],\n         [0.190894  , 0.14402887, 0.1385398 ],\n         [0.14987634, 0.10326262, 0.10031687]],\n\n        ...,\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08103643, 0.        , 0.        ]],\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08084708, 0.        , 0.        ]],\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08065774, 0.        , 0.        ]]]], dtype=float32), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.]])).\nTraceback (most recent call last):\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 204, in generator_py_func\n    flattened_values = nest.flatten_up_to(output_types, values)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\", line 237, in flatten_up_to\n    return nest_util.flatten_up_to(\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1541, in flatten_up_to\n    return _tf_data_flatten_up_to(shallow_tree, input_tree)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1570, in _tf_data_flatten_up_to\n    _tf_data_assert_shallow_structure(shallow_tree, input_tree)\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1427, in _tf_data_assert_shallow_structure\n    raise ValueError(\n\nValueError: The two structures don't have the same sequence length. Input structure has length 2, while shallow structure has length 3.\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 206, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float64, tf.float32), but the yielded element was (array([[[[0.08128925, 0.06952454, 0.11266179],\n         [0.08197479, 0.07021008, 0.11334734],\n         [0.08235294, 0.07058824, 0.11372549],\n         ...,\n         [0.02068717, 0.01676561, 0.04529592],\n         [0.0255673 , 0.02062679, 0.05607511],\n         [0.03233111, 0.02448797, 0.07346391]],\n\n        [[0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         [0.07879915, 0.06703445, 0.1101717 ],\n         ...,\n         [0.02137272, 0.01745115, 0.04666701],\n         [0.02693839, 0.02131233, 0.05950284],\n         [0.03301665, 0.02517352, 0.07552055]],\n\n        [[0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         ...,\n         [0.02205826, 0.0181367 , 0.0480381 ],\n         [0.02830948, 0.02199788, 0.06293057],\n         [0.0337022 , 0.02585906, 0.07757718]],\n\n        ...,\n\n        [[0.08652318, 0.04068866, 0.09828788],\n         [0.09007052, 0.04576935, 0.10183523],\n         [0.12978587, 0.08162352, 0.13934353],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]],\n\n        [[0.08446655, 0.03931756, 0.09623125],\n         [0.09692597, 0.05193925, 0.10869068],\n         [0.13732687, 0.08847897, 0.14551343],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]],\n\n        [[0.08240991, 0.03794647, 0.09417462],\n         [0.10378142, 0.05810916, 0.11554613],\n         [0.14486787, 0.09533442, 0.15168333],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]]],\n\n\n       [[[0.4249347 , 0.34504914, 0.3634233 ],\n         [0.42071962, 0.3416298 , 0.35952672],\n         [0.42186275, 0.34188476, 0.35930443],\n         ...,\n         [0.6831743 , 0.54199785, 0.58121353],\n         [0.68365157, 0.5424751 , 0.5816908 ],\n         [0.6841288 , 0.54295236, 0.58216804]],\n\n        [[0.41059265, 0.3360245 , 0.35173663],\n         [0.4100829 , 0.3355731 , 0.35289258],\n         [0.42132497, 0.34511667, 0.36296198],\n         ...,\n         [0.6838979 , 0.54113704, 0.5811449 ],\n         [0.6824662 , 0.5387508 , 0.579236  ],\n         [0.68103445, 0.5363646 , 0.577327  ]],\n\n        [[0.40744996, 0.33294016, 0.35137042],\n         [0.41065064, 0.33614084, 0.35705963],\n         [0.4271035 , 0.3525937 , 0.3743314 ],\n         ...,\n         [0.6905341 , 0.5415145 , 0.58465177],\n         [0.70055616, 0.55153656, 0.5946738 ],\n         [0.71057814, 0.56155854, 0.6046958 ]],\n\n        ...,\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.716917  , 0.65196496, 0.7247601 ],\n         [0.7167791 , 0.6479347 , 0.7224445 ],\n         [0.686169  , 0.61987805, 0.69438785]],\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.71453077, 0.650056  , 0.7223739 ],\n         [0.7186881 , 0.6503209 , 0.7248307 ],\n         [0.69141865, 0.6241732 , 0.698683  ]],\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.7121446 , 0.64814705, 0.71998775],\n         [0.720597  , 0.65270704, 0.72721684],\n         [0.6966683 , 0.6284684 , 0.7029782 ]]],\n\n\n       [[[0.5667194 , 0.39607844, 0.32347673],\n         [0.5663973 , 0.39607844, 0.3237988 ],\n         [0.56607527, 0.39607844, 0.32412085],\n         ...,\n         [0.9620221 , 0.8101107 , 0.76305187],\n         [0.96593046, 0.8296523 , 0.7825935 ],\n         [0.9710501 , 0.8491939 , 0.80092376]],\n\n        [[0.57262796, 0.39607844, 0.32156864],\n         [0.5719838 , 0.39607844, 0.32156864],\n         [0.57133967, 0.39607844, 0.32156864],\n         ...,\n         [0.9623442 , 0.811721  , 0.7646622 ],\n         [0.9662525 , 0.83126265, 0.7842038 ],\n         [0.97169423, 0.85080427, 0.802212  ]],\n\n        [[0.5804446 , 0.39806542, 0.32156864],\n         [0.5798004 , 0.39774334, 0.32156864],\n         [0.5791563 , 0.39742127, 0.32156864],\n         ...,\n         [0.9626663 , 0.8133314 , 0.7662726 ],\n         [0.9665746 , 0.832873  , 0.78581417],\n         [0.9723384 , 0.8524146 , 0.8035003 ]],\n\n        ...,\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.27477637, 0.23556069, 0.38065872],\n         [0.268774  , 0.22955829, 0.37465635],\n         [0.26666668, 0.22745098, 0.37254903]],\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.27413222, 0.23491654, 0.38001457],\n         [0.26845193, 0.22923622, 0.37433428],\n         [0.26666668, 0.22745098, 0.37254903]],\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.2734881 , 0.23427239, 0.37937045],\n         [0.26812986, 0.22891416, 0.3740122 ],\n         [0.26666668, 0.22745098, 0.37254903]]],\n\n\n       ...,\n\n\n       [[[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.76699823, 0.61128163, 0.47010514],\n         [0.7591694 , 0.60230666, 0.46113017],\n         [0.75134057, 0.5917167 , 0.45054024]],\n\n        [[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.7665246 , 0.61057127, 0.46939477],\n         [0.75869584, 0.6018331 , 0.46065658],\n         [0.750867  , 0.59100634, 0.44982988]],\n\n        [[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.76605105, 0.6098609 , 0.46868443],\n         [0.7582222 , 0.6013595 , 0.46018302],\n         [0.7503934 , 0.59029603, 0.4491195 ]],\n\n        ...,\n\n        [[0.5671408 , 0.6159844 , 0.6370248 ],\n         [0.558448  , 0.6129835 , 0.6404345 ],\n         [0.55991787, 0.6152005 , 0.6413666 ],\n         ...,\n         [0.60390294, 0.6509804 , 0.660775  ],\n         [0.60437655, 0.6509804 , 0.6610118 ],\n         [0.6048501 , 0.6509804 , 0.66124856]],\n\n        [[0.56798685, 0.6187748 , 0.640423  ],\n         [0.55052006, 0.607025  , 0.63298017],\n         [0.550446  , 0.60670817, 0.6321519 ],\n         ...,\n         [0.6       , 0.64901745, 0.6568606 ],\n         [0.6       , 0.64925426, 0.6570974 ],\n         [0.6       , 0.6494911 , 0.6573342 ]],\n\n        [[0.574855  , 0.624527  , 0.6463931 ],\n         [0.5522338 , 0.60807985, 0.6338604 ],\n         [0.5523607 , 0.60731983, 0.6325268 ],\n         ...,\n         [0.5980442 , 0.64510304, 0.6529462 ],\n         [0.598281  , 0.64533985, 0.653183  ],\n         [0.59851784, 0.64557666, 0.6534198 ]]],\n\n\n       [[[0.53646374, 0.3443069 , 0.21881668],\n         [0.5372549 , 0.34509805, 0.21960784],\n         [0.5372549 , 0.34509805, 0.21960784],\n         ...,\n         [0.64803743, 0.40735027, 0.23627274],\n         [0.6403957 , 0.39725846, 0.23137255],\n         [0.63529414, 0.39088678, 0.23264262]],\n\n        [[0.5340238 , 0.34186694, 0.21637674],\n         [0.5335259 , 0.341369  , 0.21587881],\n         [0.53440887, 0.34225202, 0.2167618 ],\n         ...,\n         [0.64627147, 0.4047013 , 0.23450674],\n         [0.63862973, 0.39549246, 0.23137255],\n         [0.63529414, 0.3900038 , 0.23352562]],\n\n        [[0.53843445, 0.34627756, 0.22078736],\n         [0.5369617 , 0.34480482, 0.2193146 ],\n         [0.5360787 , 0.3439218 , 0.21843162],\n         ...,\n         [0.6445055 , 0.4020523 , 0.23274076],\n         [0.63686377, 0.3937265 , 0.23137255],\n         [0.63529414, 0.38912082, 0.2344086 ]],\n\n        ...,\n\n        [[0.5497593 , 0.36788774, 0.26592696],\n         [0.56134427, 0.3809521 , 0.2789913 ],\n         [0.558745  , 0.3813333 , 0.27639204],\n         ...,\n         [0.4392157 , 0.26783058, 0.03946038],\n         [0.4392157 , 0.2651816 , 0.03592841],\n         [0.4392157 , 0.26253262, 0.03239643]],\n\n        [[0.5495928 , 0.3692006 , 0.2672398 ],\n         [0.56462073, 0.38427114, 0.28226778],\n         [0.556979  , 0.38045028, 0.27462605],\n         ...,\n         [0.45662558, 0.29089978, 0.07505538],\n         [0.45132762, 0.2847188 , 0.06534245],\n         [0.44602966, 0.27853787, 0.05562952]],\n\n        [[0.5531247 , 0.37273258, 0.2707718 ],\n         [0.56285477, 0.38338816, 0.2805018 ],\n         [0.5568628 , 0.3795673 , 0.27615955],\n         ...,\n         [0.48235172, 0.32324773, 0.11708491],\n         [0.47617078, 0.3153008 , 0.10737199],\n         [0.4699898 , 0.30735385, 0.09765906]]],\n\n\n       [[[0.00295865, 0.00295865, 0.00295865],\n         [0.003148  , 0.003148  , 0.003148  ],\n         [0.00333735, 0.00333735, 0.00333735],\n         ...,\n         [0.10675435, 0.04868467, 0.04718081],\n         [0.10609207, 0.04933306, 0.04988058],\n         [0.0921601 , 0.04314797, 0.04122626]],\n\n        [[0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         ...,\n         [0.1564236 , 0.0999303 , 0.09648446],\n         [0.14805317, 0.09609426, 0.09350255],\n         [0.12336127, 0.07512596, 0.07179263]],\n\n        [[0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         ...,\n         [0.2070707 , 0.15295699, 0.14903542],\n         [0.190894  , 0.14402887, 0.1385398 ],\n         [0.14987634, 0.10326262, 0.10031687]],\n\n        ...,\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08103643, 0.        , 0.        ]],\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08084708, 0.        , 0.        ]],\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08065774, 0.        , 0.        ]]]], dtype=float32), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.]])).\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_827617]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 326\u001b[0m\n\u001b[0;32m    319\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow(\n\u001b[0;32m    320\u001b[0m     X_train, \n\u001b[0;32m    321\u001b[0m     y_train_onehot,\n\u001b[0;32m    322\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE\n\u001b[0;32m    323\u001b[0m )\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_onehot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This works with standard ImageDataGenerator\u001b[39;49;00m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    334\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel trained and saved as\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_path)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nTypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float64, tf.float32), but the yielded element was (array([[[[0.08128925, 0.06952454, 0.11266179],\n         [0.08197479, 0.07021008, 0.11334734],\n         [0.08235294, 0.07058824, 0.11372549],\n         ...,\n         [0.02068717, 0.01676561, 0.04529592],\n         [0.0255673 , 0.02062679, 0.05607511],\n         [0.03233111, 0.02448797, 0.07346391]],\n\n        [[0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         [0.07879915, 0.06703445, 0.1101717 ],\n         ...,\n         [0.02137272, 0.01745115, 0.04666701],\n         [0.02693839, 0.02131233, 0.05950284],\n         [0.03301665, 0.02517352, 0.07552055]],\n\n        [[0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         ...,\n         [0.02205826, 0.0181367 , 0.0480381 ],\n         [0.02830948, 0.02199788, 0.06293057],\n         [0.0337022 , 0.02585906, 0.07757718]],\n\n        ...,\n\n        [[0.08652318, 0.04068866, 0.09828788],\n         [0.09007052, 0.04576935, 0.10183523],\n         [0.12978587, 0.08162352, 0.13934353],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]],\n\n        [[0.08446655, 0.03931756, 0.09623125],\n         [0.09692597, 0.05193925, 0.10869068],\n         [0.13732687, 0.08847897, 0.14551343],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]],\n\n        [[0.08240991, 0.03794647, 0.09417462],\n         [0.10378142, 0.05810916, 0.11554613],\n         [0.14486787, 0.09533442, 0.15168333],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]]],\n\n\n       [[[0.4249347 , 0.34504914, 0.3634233 ],\n         [0.42071962, 0.3416298 , 0.35952672],\n         [0.42186275, 0.34188476, 0.35930443],\n         ...,\n         [0.6831743 , 0.54199785, 0.58121353],\n         [0.68365157, 0.5424751 , 0.5816908 ],\n         [0.6841288 , 0.54295236, 0.58216804]],\n\n        [[0.41059265, 0.3360245 , 0.35173663],\n         [0.4100829 , 0.3355731 , 0.35289258],\n         [0.42132497, 0.34511667, 0.36296198],\n         ...,\n         [0.6838979 , 0.54113704, 0.5811449 ],\n         [0.6824662 , 0.5387508 , 0.579236  ],\n         [0.68103445, 0.5363646 , 0.577327  ]],\n\n        [[0.40744996, 0.33294016, 0.35137042],\n         [0.41065064, 0.33614084, 0.35705963],\n         [0.4271035 , 0.3525937 , 0.3743314 ],\n         ...,\n         [0.6905341 , 0.5415145 , 0.58465177],\n         [0.70055616, 0.55153656, 0.5946738 ],\n         [0.71057814, 0.56155854, 0.6046958 ]],\n\n        ...,\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.716917  , 0.65196496, 0.7247601 ],\n         [0.7167791 , 0.6479347 , 0.7224445 ],\n         [0.686169  , 0.61987805, 0.69438785]],\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.71453077, 0.650056  , 0.7223739 ],\n         [0.7186881 , 0.6503209 , 0.7248307 ],\n         [0.69141865, 0.6241732 , 0.698683  ]],\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.7121446 , 0.64814705, 0.71998775],\n         [0.720597  , 0.65270704, 0.72721684],\n         [0.6966683 , 0.6284684 , 0.7029782 ]]],\n\n\n       [[[0.5667194 , 0.39607844, 0.32347673],\n         [0.5663973 , 0.39607844, 0.3237988 ],\n         [0.56607527, 0.39607844, 0.32412085],\n         ...,\n         [0.9620221 , 0.8101107 , 0.76305187],\n         [0.96593046, 0.8296523 , 0.7825935 ],\n         [0.9710501 , 0.8491939 , 0.80092376]],\n\n        [[0.57262796, 0.39607844, 0.32156864],\n         [0.5719838 , 0.39607844, 0.32156864],\n         [0.57133967, 0.39607844, 0.32156864],\n         ...,\n         [0.9623442 , 0.811721  , 0.7646622 ],\n         [0.9662525 , 0.83126265, 0.7842038 ],\n         [0.97169423, 0.85080427, 0.802212  ]],\n\n        [[0.5804446 , 0.39806542, 0.32156864],\n         [0.5798004 , 0.39774334, 0.32156864],\n         [0.5791563 , 0.39742127, 0.32156864],\n         ...,\n         [0.9626663 , 0.8133314 , 0.7662726 ],\n         [0.9665746 , 0.832873  , 0.78581417],\n         [0.9723384 , 0.8524146 , 0.8035003 ]],\n\n        ...,\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.27477637, 0.23556069, 0.38065872],\n         [0.268774  , 0.22955829, 0.37465635],\n         [0.26666668, 0.22745098, 0.37254903]],\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.27413222, 0.23491654, 0.38001457],\n         [0.26845193, 0.22923622, 0.37433428],\n         [0.26666668, 0.22745098, 0.37254903]],\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.2734881 , 0.23427239, 0.37937045],\n         [0.26812986, 0.22891416, 0.3740122 ],\n         [0.26666668, 0.22745098, 0.37254903]]],\n\n\n       ...,\n\n\n       [[[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.76699823, 0.61128163, 0.47010514],\n         [0.7591694 , 0.60230666, 0.46113017],\n         [0.75134057, 0.5917167 , 0.45054024]],\n\n        [[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.7665246 , 0.61057127, 0.46939477],\n         [0.75869584, 0.6018331 , 0.46065658],\n         [0.750867  , 0.59100634, 0.44982988]],\n\n        [[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.76605105, 0.6098609 , 0.46868443],\n         [0.7582222 , 0.6013595 , 0.46018302],\n         [0.7503934 , 0.59029603, 0.4491195 ]],\n\n        ...,\n\n        [[0.5671408 , 0.6159844 , 0.6370248 ],\n         [0.558448  , 0.6129835 , 0.6404345 ],\n         [0.55991787, 0.6152005 , 0.6413666 ],\n         ...,\n         [0.60390294, 0.6509804 , 0.660775  ],\n         [0.60437655, 0.6509804 , 0.6610118 ],\n         [0.6048501 , 0.6509804 , 0.66124856]],\n\n        [[0.56798685, 0.6187748 , 0.640423  ],\n         [0.55052006, 0.607025  , 0.63298017],\n         [0.550446  , 0.60670817, 0.6321519 ],\n         ...,\n         [0.6       , 0.64901745, 0.6568606 ],\n         [0.6       , 0.64925426, 0.6570974 ],\n         [0.6       , 0.6494911 , 0.6573342 ]],\n\n        [[0.574855  , 0.624527  , 0.6463931 ],\n         [0.5522338 , 0.60807985, 0.6338604 ],\n         [0.5523607 , 0.60731983, 0.6325268 ],\n         ...,\n         [0.5980442 , 0.64510304, 0.6529462 ],\n         [0.598281  , 0.64533985, 0.653183  ],\n         [0.59851784, 0.64557666, 0.6534198 ]]],\n\n\n       [[[0.53646374, 0.3443069 , 0.21881668],\n         [0.5372549 , 0.34509805, 0.21960784],\n         [0.5372549 , 0.34509805, 0.21960784],\n         ...,\n         [0.64803743, 0.40735027, 0.23627274],\n         [0.6403957 , 0.39725846, 0.23137255],\n         [0.63529414, 0.39088678, 0.23264262]],\n\n        [[0.5340238 , 0.34186694, 0.21637674],\n         [0.5335259 , 0.341369  , 0.21587881],\n         [0.53440887, 0.34225202, 0.2167618 ],\n         ...,\n         [0.64627147, 0.4047013 , 0.23450674],\n         [0.63862973, 0.39549246, 0.23137255],\n         [0.63529414, 0.3900038 , 0.23352562]],\n\n        [[0.53843445, 0.34627756, 0.22078736],\n         [0.5369617 , 0.34480482, 0.2193146 ],\n         [0.5360787 , 0.3439218 , 0.21843162],\n         ...,\n         [0.6445055 , 0.4020523 , 0.23274076],\n         [0.63686377, 0.3937265 , 0.23137255],\n         [0.63529414, 0.38912082, 0.2344086 ]],\n\n        ...,\n\n        [[0.5497593 , 0.36788774, 0.26592696],\n         [0.56134427, 0.3809521 , 0.2789913 ],\n         [0.558745  , 0.3813333 , 0.27639204],\n         ...,\n         [0.4392157 , 0.26783058, 0.03946038],\n         [0.4392157 , 0.2651816 , 0.03592841],\n         [0.4392157 , 0.26253262, 0.03239643]],\n\n        [[0.5495928 , 0.3692006 , 0.2672398 ],\n         [0.56462073, 0.38427114, 0.28226778],\n         [0.556979  , 0.38045028, 0.27462605],\n         ...,\n         [0.45662558, 0.29089978, 0.07505538],\n         [0.45132762, 0.2847188 , 0.06534245],\n         [0.44602966, 0.27853787, 0.05562952]],\n\n        [[0.5531247 , 0.37273258, 0.2707718 ],\n         [0.56285477, 0.38338816, 0.2805018 ],\n         [0.5568628 , 0.3795673 , 0.27615955],\n         ...,\n         [0.48235172, 0.32324773, 0.11708491],\n         [0.47617078, 0.3153008 , 0.10737199],\n         [0.4699898 , 0.30735385, 0.09765906]]],\n\n\n       [[[0.00295865, 0.00295865, 0.00295865],\n         [0.003148  , 0.003148  , 0.003148  ],\n         [0.00333735, 0.00333735, 0.00333735],\n         ...,\n         [0.10675435, 0.04868467, 0.04718081],\n         [0.10609207, 0.04933306, 0.04988058],\n         [0.0921601 , 0.04314797, 0.04122626]],\n\n        [[0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         ...,\n         [0.1564236 , 0.0999303 , 0.09648446],\n         [0.14805317, 0.09609426, 0.09350255],\n         [0.12336127, 0.07512596, 0.07179263]],\n\n        [[0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         ...,\n         [0.2070707 , 0.15295699, 0.14903542],\n         [0.190894  , 0.14402887, 0.1385398 ],\n         [0.14987634, 0.10326262, 0.10031687]],\n\n        ...,\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08103643, 0.        , 0.        ]],\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08084708, 0.        , 0.        ]],\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08065774, 0.        , 0.        ]]]], dtype=float32), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.]])).\nTraceback (most recent call last):\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 204, in generator_py_func\n    flattened_values = nest.flatten_up_to(output_types, values)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\", line 237, in flatten_up_to\n    return nest_util.flatten_up_to(\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1541, in flatten_up_to\n    return _tf_data_flatten_up_to(shallow_tree, input_tree)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1570, in _tf_data_flatten_up_to\n    _tf_data_assert_shallow_structure(shallow_tree, input_tree)\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py\", line 1427, in _tf_data_assert_shallow_structure\n    raise ValueError(\n\nValueError: The two structures don't have the same sequence length. Input structure has length 2, while shallow structure has length 3.\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\juesh\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 206, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float64, tf.float32), but the yielded element was (array([[[[0.08128925, 0.06952454, 0.11266179],\n         [0.08197479, 0.07021008, 0.11334734],\n         [0.08235294, 0.07058824, 0.11372549],\n         ...,\n         [0.02068717, 0.01676561, 0.04529592],\n         [0.0255673 , 0.02062679, 0.05607511],\n         [0.03233111, 0.02448797, 0.07346391]],\n\n        [[0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         [0.07879915, 0.06703445, 0.1101717 ],\n         ...,\n         [0.02137272, 0.01745115, 0.04666701],\n         [0.02693839, 0.02131233, 0.05950284],\n         [0.03301665, 0.02517352, 0.07552055]],\n\n        [[0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         [0.07843138, 0.06666667, 0.10980392],\n         ...,\n         [0.02205826, 0.0181367 , 0.0480381 ],\n         [0.02830948, 0.02199788, 0.06293057],\n         [0.0337022 , 0.02585906, 0.07757718]],\n\n        ...,\n\n        [[0.08652318, 0.04068866, 0.09828788],\n         [0.09007052, 0.04576935, 0.10183523],\n         [0.12978587, 0.08162352, 0.13934353],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]],\n\n        [[0.08446655, 0.03931756, 0.09623125],\n         [0.09692597, 0.05193925, 0.10869068],\n         [0.13732687, 0.08847897, 0.14551343],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]],\n\n        [[0.08240991, 0.03794647, 0.09417462],\n         [0.10378142, 0.05810916, 0.11554613],\n         [0.14486787, 0.09533442, 0.15168333],\n         ...,\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569],\n         [0.03529412, 0.04313726, 0.03921569]]],\n\n\n       [[[0.4249347 , 0.34504914, 0.3634233 ],\n         [0.42071962, 0.3416298 , 0.35952672],\n         [0.42186275, 0.34188476, 0.35930443],\n         ...,\n         [0.6831743 , 0.54199785, 0.58121353],\n         [0.68365157, 0.5424751 , 0.5816908 ],\n         [0.6841288 , 0.54295236, 0.58216804]],\n\n        [[0.41059265, 0.3360245 , 0.35173663],\n         [0.4100829 , 0.3355731 , 0.35289258],\n         [0.42132497, 0.34511667, 0.36296198],\n         ...,\n         [0.6838979 , 0.54113704, 0.5811449 ],\n         [0.6824662 , 0.5387508 , 0.579236  ],\n         [0.68103445, 0.5363646 , 0.577327  ]],\n\n        [[0.40744996, 0.33294016, 0.35137042],\n         [0.41065064, 0.33614084, 0.35705963],\n         [0.4271035 , 0.3525937 , 0.3743314 ],\n         ...,\n         [0.6905341 , 0.5415145 , 0.58465177],\n         [0.70055616, 0.55153656, 0.5946738 ],\n         [0.71057814, 0.56155854, 0.6046958 ]],\n\n        ...,\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.716917  , 0.65196496, 0.7247601 ],\n         [0.7167791 , 0.6479347 , 0.7224445 ],\n         [0.686169  , 0.61987805, 0.69438785]],\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.71453077, 0.650056  , 0.7223739 ],\n         [0.7186881 , 0.6503209 , 0.7248307 ],\n         [0.69141865, 0.6241732 , 0.698683  ]],\n\n        [[0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         [0.3764706 , 0.25882354, 0.30588236],\n         ...,\n         [0.7121446 , 0.64814705, 0.71998775],\n         [0.720597  , 0.65270704, 0.72721684],\n         [0.6966683 , 0.6284684 , 0.7029782 ]]],\n\n\n       [[[0.5667194 , 0.39607844, 0.32347673],\n         [0.5663973 , 0.39607844, 0.3237988 ],\n         [0.56607527, 0.39607844, 0.32412085],\n         ...,\n         [0.9620221 , 0.8101107 , 0.76305187],\n         [0.96593046, 0.8296523 , 0.7825935 ],\n         [0.9710501 , 0.8491939 , 0.80092376]],\n\n        [[0.57262796, 0.39607844, 0.32156864],\n         [0.5719838 , 0.39607844, 0.32156864],\n         [0.57133967, 0.39607844, 0.32156864],\n         ...,\n         [0.9623442 , 0.811721  , 0.7646622 ],\n         [0.9662525 , 0.83126265, 0.7842038 ],\n         [0.97169423, 0.85080427, 0.802212  ]],\n\n        [[0.5804446 , 0.39806542, 0.32156864],\n         [0.5798004 , 0.39774334, 0.32156864],\n         [0.5791563 , 0.39742127, 0.32156864],\n         ...,\n         [0.9626663 , 0.8133314 , 0.7662726 ],\n         [0.9665746 , 0.832873  , 0.78581417],\n         [0.9723384 , 0.8524146 , 0.8035003 ]],\n\n        ...,\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.27477637, 0.23556069, 0.38065872],\n         [0.268774  , 0.22955829, 0.37465635],\n         [0.26666668, 0.22745098, 0.37254903]],\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.27413222, 0.23491654, 0.38001457],\n         [0.26845193, 0.22923622, 0.37433428],\n         [0.26666668, 0.22745098, 0.37254903]],\n\n        [[0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         [0.81960785, 0.8352941 , 0.8784314 ],\n         ...,\n         [0.2734881 , 0.23427239, 0.37937045],\n         [0.26812986, 0.22891416, 0.3740122 ],\n         [0.26666668, 0.22745098, 0.37254903]]],\n\n\n       ...,\n\n\n       [[[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.76699823, 0.61128163, 0.47010514],\n         [0.7591694 , 0.60230666, 0.46113017],\n         [0.75134057, 0.5917167 , 0.45054024]],\n\n        [[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.7665246 , 0.61057127, 0.46939477],\n         [0.75869584, 0.6018331 , 0.46065658],\n         [0.750867  , 0.59100634, 0.44982988]],\n\n        [[0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         [0.6627451 , 0.50980395, 0.35686275],\n         ...,\n         [0.76605105, 0.6098609 , 0.46868443],\n         [0.7582222 , 0.6013595 , 0.46018302],\n         [0.7503934 , 0.59029603, 0.4491195 ]],\n\n        ...,\n\n        [[0.5671408 , 0.6159844 , 0.6370248 ],\n         [0.558448  , 0.6129835 , 0.6404345 ],\n         [0.55991787, 0.6152005 , 0.6413666 ],\n         ...,\n         [0.60390294, 0.6509804 , 0.660775  ],\n         [0.60437655, 0.6509804 , 0.6610118 ],\n         [0.6048501 , 0.6509804 , 0.66124856]],\n\n        [[0.56798685, 0.6187748 , 0.640423  ],\n         [0.55052006, 0.607025  , 0.63298017],\n         [0.550446  , 0.60670817, 0.6321519 ],\n         ...,\n         [0.6       , 0.64901745, 0.6568606 ],\n         [0.6       , 0.64925426, 0.6570974 ],\n         [0.6       , 0.6494911 , 0.6573342 ]],\n\n        [[0.574855  , 0.624527  , 0.6463931 ],\n         [0.5522338 , 0.60807985, 0.6338604 ],\n         [0.5523607 , 0.60731983, 0.6325268 ],\n         ...,\n         [0.5980442 , 0.64510304, 0.6529462 ],\n         [0.598281  , 0.64533985, 0.653183  ],\n         [0.59851784, 0.64557666, 0.6534198 ]]],\n\n\n       [[[0.53646374, 0.3443069 , 0.21881668],\n         [0.5372549 , 0.34509805, 0.21960784],\n         [0.5372549 , 0.34509805, 0.21960784],\n         ...,\n         [0.64803743, 0.40735027, 0.23627274],\n         [0.6403957 , 0.39725846, 0.23137255],\n         [0.63529414, 0.39088678, 0.23264262]],\n\n        [[0.5340238 , 0.34186694, 0.21637674],\n         [0.5335259 , 0.341369  , 0.21587881],\n         [0.53440887, 0.34225202, 0.2167618 ],\n         ...,\n         [0.64627147, 0.4047013 , 0.23450674],\n         [0.63862973, 0.39549246, 0.23137255],\n         [0.63529414, 0.3900038 , 0.23352562]],\n\n        [[0.53843445, 0.34627756, 0.22078736],\n         [0.5369617 , 0.34480482, 0.2193146 ],\n         [0.5360787 , 0.3439218 , 0.21843162],\n         ...,\n         [0.6445055 , 0.4020523 , 0.23274076],\n         [0.63686377, 0.3937265 , 0.23137255],\n         [0.63529414, 0.38912082, 0.2344086 ]],\n\n        ...,\n\n        [[0.5497593 , 0.36788774, 0.26592696],\n         [0.56134427, 0.3809521 , 0.2789913 ],\n         [0.558745  , 0.3813333 , 0.27639204],\n         ...,\n         [0.4392157 , 0.26783058, 0.03946038],\n         [0.4392157 , 0.2651816 , 0.03592841],\n         [0.4392157 , 0.26253262, 0.03239643]],\n\n        [[0.5495928 , 0.3692006 , 0.2672398 ],\n         [0.56462073, 0.38427114, 0.28226778],\n         [0.556979  , 0.38045028, 0.27462605],\n         ...,\n         [0.45662558, 0.29089978, 0.07505538],\n         [0.45132762, 0.2847188 , 0.06534245],\n         [0.44602966, 0.27853787, 0.05562952]],\n\n        [[0.5531247 , 0.37273258, 0.2707718 ],\n         [0.56285477, 0.38338816, 0.2805018 ],\n         [0.5568628 , 0.3795673 , 0.27615955],\n         ...,\n         [0.48235172, 0.32324773, 0.11708491],\n         [0.47617078, 0.3153008 , 0.10737199],\n         [0.4699898 , 0.30735385, 0.09765906]]],\n\n\n       [[[0.00295865, 0.00295865, 0.00295865],\n         [0.003148  , 0.003148  , 0.003148  ],\n         [0.00333735, 0.00333735, 0.00333735],\n         ...,\n         [0.10675435, 0.04868467, 0.04718081],\n         [0.10609207, 0.04933306, 0.04988058],\n         [0.0921601 , 0.04314797, 0.04122626]],\n\n        [[0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         ...,\n         [0.1564236 , 0.0999303 , 0.09648446],\n         [0.14805317, 0.09609426, 0.09350255],\n         [0.12336127, 0.07512596, 0.07179263]],\n\n        [[0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         [0.        , 0.        , 0.        ],\n         ...,\n         [0.2070707 , 0.15295699, 0.14903542],\n         [0.190894  , 0.14402887, 0.1385398 ],\n         [0.14987634, 0.10326262, 0.10031687]],\n\n        ...,\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08103643, 0.        , 0.        ]],\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08084708, 0.        , 0.        ]],\n\n        [[0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         [0.02352941, 0.        , 0.00784314],\n         ...,\n         [0.08235294, 0.        , 0.        ],\n         [0.08235294, 0.        , 0.        ],\n         [0.08065774, 0.        , 0.        ]]]], dtype=float32), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0.]])).\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_827617]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input, GlobalAveragePooling2D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ========================================================\n",
    "# 1. Configuration Settings\n",
    "# ========================================================\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Image parameters\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Paths\n",
    "train_csv_path = 'train.csv'\n",
    "test_csv_path = 'test.csv'\n",
    "base_image_dir = './train/train'\n",
    "test_directory = './test/test'\n",
    "model_path = 'skin_condition_model_improved.keras'  # Changed from .h5 to .keras\n",
    "\n",
    "# ========================================================\n",
    "# 2. Data Augmentation Functions\n",
    "# ========================================================\n",
    "def augment_image(img):\n",
    "    \"\"\"Apply random augmentations to a single image\"\"\"\n",
    "    # Convert PIL Image to NumPy array if needed\n",
    "    if isinstance(img, Image.Image):\n",
    "        img_array = np.array(img)\n",
    "    else:\n",
    "        img_array = img.copy()\n",
    "    \n",
    "    # Random flip\n",
    "    if random.random() > 0.5:\n",
    "        img_array = np.fliplr(img_array)\n",
    "    \n",
    "    # Random rotation (convert back to PIL for rotation)\n",
    "    if random.random() > 0.5:\n",
    "        angle = random.uniform(-20, 20)\n",
    "        img_pil = Image.fromarray(img_array.astype('uint8'))\n",
    "        img_pil = img_pil.rotate(angle, resample=Image.BILINEAR, expand=False)\n",
    "        img_array = np.array(img_pil)\n",
    "    \n",
    "    # Random brightness and contrast (convert to PIL)\n",
    "    if random.random() > 0.5:\n",
    "        img_pil = Image.fromarray(img_array.astype('uint8'))\n",
    "        enhancer = ImageEnhance.Brightness(img_pil)\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        img_pil = enhancer.enhance(factor)\n",
    "        \n",
    "        enhancer = ImageEnhance.Contrast(img_pil)\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        img_pil = enhancer.enhance(factor)\n",
    "        \n",
    "        img_array = np.array(img_pil)\n",
    "        \n",
    "    return img_array\n",
    "\n",
    "# ========================================================\n",
    "# 3. Load and Clean CSV Data\n",
    "# ========================================================\n",
    "print(\"Loading and cleaning CSV data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Add '.jpg' extension to md5hash\n",
    "train_df['md5hash'] = train_df['md5hash'].astype(str) + '.jpg'\n",
    "test_df['md5hash'] = test_df['md5hash'].astype(str) + '.jpg'\n",
    "\n",
    "# Combine label and md5hash to form the file path (e.g. \"acne/xyz.jpg\")\n",
    "train_df['file_path'] = train_df['label'] + '/' + train_df['md5hash']\n",
    "\n",
    "# Replace ddi_scale with fitzpatrick_centaur\n",
    "train_df['fitzpatrick_scale'] = train_df['fitzpatrick_centaur']\n",
    "test_df['fitzpatrick_scale'] = test_df['fitzpatrick_centaur']\n",
    "train_df.drop(columns=['ddi_scale'], inplace=True)\n",
    "test_df.drop(columns=['ddi_scale'], inplace=True)\n",
    "\n",
    "# Remove rows with wrongly labelled data\n",
    "train_df = train_df[train_df['qc'] != '3 Wrongly labelled']\n",
    "test_df = test_df[test_df['qc'] != '3 Wrongly labelled']\n",
    "\n",
    "# Encode the label column numerically\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label_numerical'] = label_encoder.fit_transform(train_df['label'])\n",
    "label_mapping = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
    "inverse_label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "\n",
    "print(f\"Number of classes: {len(label_mapping)}\")\n",
    "print(f\"Label mapping: {label_mapping}\")\n",
    "\n",
    "# Encode fitzpatrick scale as numerical to use as additional feature\n",
    "train_df['fitzpatrick_numerical'] = train_df['fitzpatrick_scale'].fillna(0).astype(int)\n",
    "test_df['fitzpatrick_numerical'] = test_df['fitzpatrick_scale'].fillna(0).astype(int)\n",
    "\n",
    "# Check class distribution\n",
    "class_counts = train_df['label_numerical'].value_counts()\n",
    "print(\"Class distribution:\\n\", class_counts)\n",
    "\n",
    "# ========================================================\n",
    "# 4. Build the Dataset in Memory\n",
    "# ========================================================\n",
    "print(\"Loading and processing training images...\")\n",
    "all_images = []\n",
    "all_labels = []\n",
    "all_fitzpatrick = []\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    file_path = os.path.join(base_image_dir, row['file_path'])\n",
    "    \n",
    "    # Load and resize image\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                img = img.resize(IMAGE_SIZE)\n",
    "                img = img.convert('RGB')\n",
    "                img_array = np.array(img, dtype=np.float32)\n",
    "                \n",
    "                # Normalize pixel values to [0,1]\n",
    "                img_array = img_array / 255.0\n",
    "                \n",
    "                all_images.append(img_array)\n",
    "                all_labels.append(row['label_numerical'])\n",
    "                all_fitzpatrick.append(row['fitzpatrick_numerical'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: File not found {file_path}\")\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(all_images, dtype=np.float32)\n",
    "y = np.array(all_labels, dtype=np.int32)\n",
    "fitzpatrick_data = np.array(all_fitzpatrick, dtype=np.float32)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_onehot = to_categorical(y, num_classes=len(label_mapping))\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"Shape of fitzpatrick_data:\", fitzpatrick_data.shape)\n",
    "\n",
    "# ========================================================\n",
    "# 5. Split into Training and Validation Sets\n",
    "# ========================================================\n",
    "X_train, X_val, y_train, y_val, fitz_train, fitz_val = train_test_split(\n",
    "    X, y, fitzpatrick_data, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# One-hot encoded versions for categorical loss\n",
    "y_train_onehot = to_categorical(y_train, num_classes=len(label_mapping))\n",
    "y_val_onehot = to_categorical(y_val, num_classes=len(label_mapping))\n",
    "\n",
    "# ========================================================\n",
    "# 6. Data Augmentation Setup\n",
    "# ========================================================\n",
    "print(\"Setting up data augmentation...\")\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Calculate class weights to handle imbalance\n",
    "class_weight_dict = {i: np.float32(weight) for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "BATCH_SIZE = 16  # Reduced from 32\n",
    "\n",
    "# 5. Try training without the generator first to isolate the issue\n",
    "try:\n",
    "    # First attempt without generator to isolate the issue\n",
    "    simple_history = model.fit(\n",
    "        X_train, \n",
    "        y_train_onehot,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=2,  # Just a couple epochs to test\n",
    "        validation_data=(X_val, y_val_onehot),\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Direct training successful, now trying with generator...\")\n",
    "    \n",
    "    # Now try with generator\n",
    "    train_generator = datagen.flow(\n",
    "        X_train, \n",
    "        y_train_onehot,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_val, y_val_onehot),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "    \n",
    "    # If still failing, try without class weights\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            X_train, \n",
    "            y_train_onehot,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=(X_val, y_val_onehot),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Training still failing: {e}\")\n",
    "        \n",
    "# ========================================================\n",
    "# 7. Build the Model without Multiple Inputs\n",
    "# ========================================================\n",
    "print(\"Building simplified model with transfer learning...\")\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    # Load pre-trained EfficientNetB3 without top layers\n",
    "    base_model = EfficientNetB3(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers\n",
    "    for layer in base_model.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Image input\n",
    "    img_input = Input(shape=input_shape)\n",
    "    x = base_model(img_input)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=img_input, outputs=output)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_model(\n",
    "    input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
    "    num_classes=len(label_mapping)\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ========================================================\n",
    "# 8. Train with Callbacks\n",
    "# ========================================================\n",
    "print(\"Training model...\")\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        model_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Prepare data for training with datagen\n",
    "train_generator = datagen.flow(\n",
    "    X_train, \n",
    "    y_train_onehot,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val_onehot),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,  # This works with standard ImageDataGenerator\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Model trained and saved as\", model_path)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "print(\"Training history saved as training_history.png\")\n",
    "\n",
    "# ========================================================\n",
    "# 9. Load Best Model and Make Predictions\n",
    "# ========================================================\n",
    "print(\"Loading best model for predictions...\")\n",
    "best_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# ========================================================\n",
    "# 10. Process Test Data\n",
    "# ========================================================\n",
    "print(\"Processing test data...\")\n",
    "# Get list of all image files in the test directory\n",
    "image_files = [file for file in os.listdir(test_directory) if file.endswith('.jpg')]\n",
    "print(f\"Total image files in test directory: {len(image_files)}\")\n",
    "\n",
    "# Find any discrepancies between directory and CSV\n",
    "test_file_set = set(test_df['md5hash'])\n",
    "directory_file_set = set(image_files)\n",
    "missing_files = directory_file_set - test_file_set\n",
    "extra_files = test_file_set - directory_file_set\n",
    "\n",
    "print(f\"Files in directory but not in CSV: {missing_files}\")\n",
    "print(f\"Files in CSV but not in directory: {extra_files}\")\n",
    "\n",
    "# Process all images in the test directory\n",
    "test_images = []\n",
    "test_md5hashes = []\n",
    "\n",
    "# First process entries in the test_df\n",
    "for idx, row in test_df.iterrows():\n",
    "    file_path = os.path.join(test_directory, row['md5hash'])\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                img = img.resize(IMAGE_SIZE)\n",
    "                img = img.convert('RGB')\n",
    "                img_array = np.array(img, dtype=np.float32)\n",
    "                img_array /= 255.0\n",
    "                \n",
    "                test_images.append(img_array)\n",
    "                test_md5hashes.append(row['md5hash'].replace('.jpg', ''))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: Test file not found {file_path}\")\n",
    "\n",
    "# Then process any additional files found in the directory but not in the CSV\n",
    "for filename in missing_files:\n",
    "    file_path = os.path.join(test_directory, filename)\n",
    "    try:\n",
    "        with Image.open(file_path) as img:\n",
    "            img = img.resize(IMAGE_SIZE)\n",
    "            img = img.convert('RGB')\n",
    "            img_array = np.array(img, dtype=np.float32)\n",
    "            img_array /= 255.0\n",
    "            \n",
    "            test_images.append(img_array)\n",
    "            test_md5hashes.append(filename.replace('.jpg', ''))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing additional file {file_path}: {e}\")\n",
    "\n",
    "X_test = np.array(test_images, dtype=np.float32)\n",
    "print(f\"Final processed test images: {len(test_images)}\")\n",
    "\n",
    "# ========================================================\n",
    "# 11. Test Time Augmentation\n",
    "# ========================================================\n",
    "print(\"Generating predictions with Test Time Augmentation...\")\n",
    "\n",
    "def tta_predict(model, img_array, num_augments=5):\n",
    "    \"\"\"Test time augmentation for more robust predictions\"\"\"\n",
    "    preds = []\n",
    "    \n",
    "    # Original prediction\n",
    "    pred = model.predict(np.expand_dims(img_array, axis=0))[0]\n",
    "    preds.append(pred)\n",
    "    \n",
    "    # Get predictions with augmentations\n",
    "    for _ in range(num_augments):\n",
    "        aug_img = augment_image(img_array)\n",
    "        aug_pred = model.predict(np.expand_dims(aug_img, axis=0))[0]\n",
    "        preds.append(aug_pred)\n",
    "    \n",
    "    # Average predictions\n",
    "    avg_pred = np.mean(preds, axis=0)\n",
    "    return avg_pred\n",
    "\n",
    "# Apply TTA to each test image\n",
    "all_predictions = []\n",
    "for i in range(len(X_test)):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing test image {i}/{len(X_test)}\")\n",
    "    \n",
    "    pred = tta_predict(best_model, X_test[i])\n",
    "    all_predictions.append(pred)\n",
    "\n",
    "predictions = np.array(all_predictions)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Convert numerical labels back to original class names\n",
    "predicted_labels = [label_mapping[cls] for cls in predicted_classes]\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'md5hash': test_md5hashes,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# Save to CSV without index\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Predictions saved to submission.csv\")\n",
    "\n",
    "# ========================================================\n",
    "# 12. Evaluate Model Performance\n",
    "# ========================================================\n",
    "print(\"Evaluating model performance...\")\n",
    "# Evaluate on validation set\n",
    "val_preds = best_model.predict(X_val)\n",
    "val_pred_classes = np.argmax(val_preds, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "val_accuracy = np.mean(val_pred_classes == y_val)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, val_pred_classes, target_names=list(label_mapping.values())))\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, val_pred_classes)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nTraining and prediction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found in the test set: 1226\n",
      "Expected number of images in the test set: 1226\n",
      "Number of image files in './test/test': 1227\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of images found in the test set: {len(test_images)}\")\n",
    "print(f\"Expected number of images in the test set: {len(test_df)}\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the path to the test directory\n",
    "test_directory = './test/test'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "all_files = os.listdir(test_directory)\n",
    "\n",
    "# Filter the list to include only image files (e.g., '.jpg' files)\n",
    "image_files = [file for file in all_files if file.endswith('.jpg')]\n",
    "\n",
    "# Print the number of image files\n",
    "print(f\"Number of image files in '{test_directory}': {len(image_files)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing image files: []\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate files found: []\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
